This work reproduces and extends the implementation of the paper "Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness (BLOOD)". (https://github.com/fjelenic/between-layer-ood) The BLOOD repository includes PyTorch, the HuggingFace Transformers library, and standard data handling packages. The original setup utilized eight text classification datasets with RoBERTa and ELECTRA models as the primary architectures. The performance was measured using AUROC to compare against baselines such as MSP, Entropy, Gradient norm, and Mahalanobis Distance. I conducted new experiments with the DeBERTa-v3 model, and introduced a lightweight dataset configuration to evaluate performance under constrained settings.
To explore the adaptability of BLOOD to new architectures and settings, I modified model and dataset. The pre-trained DeBERTa-v3 model was integrated into the pipeline. As DeBERTaâ€™s architecture differs from RoBERTa/ELECTRA, specific changes were required to access intermediate layers and handle its encoder structure. This included modifying how representations are collected and how layer-wise Jacobians are computed. Special attention was paid to the [CLS] token extraction process, as DeBERTa-V3 handles special tokens differently than the original models. Among the eight available datasets in the BLOOD benchmark, the SUBJ (Subjectivity) dataset was selected and reduced to 5,000 samples for faster iterations and evaluation.
